---
layout: post
title: "Everything You've ever wanted to know about Wu-tang Lyrics"
date: "2019-01-02"
comments: true
maps: true
---
-   [Deep dive into Wu-Tang Lyrics](#deep-dive-into-wu-tang-lyrics)
-   [Find all the links on a page and extract them](#find-all-the-links-on-a-page-and-extract-them)
-   [Next we pipe in that list of links and download the text for each](#next-we-pipe-in-that-list-of-links-and-download-the-text-for-each)
-   [Start making word clouds with ggwordcloud](#start-making-word-clouds-with-ggwordcloud)
-   [TF-IDF Wordclouds by Song](#tf-idf-wordclouds-by-song)
-   [Visualize correlated networks of lyrics by using the phi coefficient](#visualize-correlated-networks-of-lyrics-by-using-the-phi-coefficient)
-   [Analyzing the sentiment of wu-tang songs](#analyzing-the-sentiment-of-wu-tang-songs)
-   [Named entity extraction: locations and people referenced in Wu-tang's songs](#named-entity-extraction-locations-and-people-referenced-in-wu-tangs-songs)
-   [A function that extracts the predicted probabilities and returns a nice dataframe](#a-function-that-extracts-the-predicted-probabilities-and-returns-a-nice-dataframe)
-   [Plotting locations and people vs. confidence in predictions](#plotting-locations-and-people-vs.-confidence-in-predictions)
-   [Plotting locations mentioned by Wu-tang on a map](#plotting-locations-mentioned-by-wu-tang-on-a-map)

Deep dive into Wu-Tang Lyrics
=============================

In this post I wanted to develop some of the techniques used in the previous blog post where we examined Nostradamus' prophecies. The subject of this analysis will be 64 Wu-Tang songs that were scraped from [here](www.metrolyrics.com) (azlyrics.com banned my IP after I tested out my code on 200+ Wu-tang songs...).

By the way, in the spirit of Jonathan Haidt's new book 'The Coddling of the American Mind,' I'm going to go ahead and issue a "trigger warning" for those of you who aren't already familiar with Wu-Tang's lyrics. After all, there is a reason why my friend's mom confiscated our 36 Chambers cassette tape in the 6th grade--expect a teeny bit of coarse language in what follows.

Again, one of my goals is to automate the process of extracting and visualizing named entities. In particular, I think the most interesting entities are names and locations. In a future post maybe I'll try out my Wikipedia idea. Anyhow, let's dive in. This time I'll show you my approach to getting the lyrics from a website.

Find all the links on a page and extract them
=============================================

The key to doing this is spending some time and examining the way the urls are formed. Look for patterns--paste0 will be your friend here. Once you have a list of song titles you can turn into nicely formed URLs, you're ready to move on.

``` r
base <- 'http://www.metrolyrics.com/wu-tang-clan-lyrics.html'

links <- read_html(base)
extract_link = html_text(html_nodes(links, 'td a'))

extract_link <- lapply(extract_link, function (x) {
  x <- tolower(x)
  x <- str_remove_all(x, '[[:punct:]]')
  x <- str_replace_all(x, '\\s+', ' ')
  x <- x[x!= '']
  x <- str_trim(x, 'left')
  x <- str_replace_all(x, '\\s', '-')
})

#get rid of 0 char ones
extract_link <- extract_link[lapply(extract_link, length)>0]
```

Next we pipe in that list of links and download the text for each
=================================================================

It's important here to use something like tryCatch for when the url you paste in isn't valid. I also suggest printing the URL you're trying to verify it makes sense. Also the SelectorGadget tool for Chrome is a lifesaver when you are trying to figure out what to pass to html\_nodes().

Note here the function was mostly created through trial and error, and there is still a tiny bit of manual cleaning to do afterwards, so it's not perfect.

``` r
#METRO LYRICS VERSION
df_lyrics <- function(list_titles){
  full_df <- data.frame(stringsAsFactors = F)
  base_url <- 'http://www.metrolyrics.com/'
  for (title in list_titles){
    web = paste0(base_url, title, 'wutang-clan.html')
    print(web)
    all_info = tryCatch(read_html(web), error= function(e) e, finally=print('OK'))
    text = tryCatch(html_text(html_nodes(all_info,'div p')), error= function(e) e, finally=print('OK'))
    text = text[text != ""]
    text = text[2:(length(text)-2)] #remove extraneous text at beginning and end
    text = str_replace_all(text, '\\s+', ' ') 
    text = str_replace_all(text, '[[:punct:]]', ' ')
    keep_el = which(nchar(text) > 50) #we picked up some junk stuff. keep big text parts only
    text = text[keep_el]
    d <- data.frame(song = title, lyrics=text) #make the name of the song and the text for later analysis
    full_df <- rbind(d, full_df)
  }
  full_df
}

#here's our dataframe with the song names and 
f <- df_lyrics(extract_link)

#write it to a csv for analysis
write.csv(f, 'wutang.csv', row.names = F)
```

Start making word clouds with ggwordcloud
=========================================

At first I was skeptical for why we needed a ggplot version of wordcloud. Then I realized you can facet and use gridArrange! For these reasons alone, I'll probably never go back to the basic wordclouds.

I also suggest lemmatizing your words to reduce the number of unique words. It just makes plotting better when you don't have more than 150-200 words.

``` r
#64 songs for now
df <- read.csv('wutang.csv', stringsAsFactors = F)

#BASIC WORD CLOUD
word <- df%>%
  mutate(lyrics = lemmatize_strings(lyrics))%>%
  unnest_tokens(word, lyrics)%>%
  anti_join(stop_words)%>%
  filter(!word %in% tolower(song))%>%
  filter(nchar(word) > 2)%>%
  count(word, sort=T)%>%
  slice(1:150)%>%
  ggplot(aes(label=word, size=n,color=log(n)))+
  geom_text_wordcloud_area()+
  scale_color_continuous(low='yellow', high='black')+ #killa beez
  theme_minimal()+
  labs(title='Most frequent words in Wu-tang songs')
```

``` r
trigram <- df%>%
  mutate(lyrics = removeWords(lyrics, c(stop_words$word,
                                        'tang', 'wu', 'yo i',
                                        'i i', 'i', 'ain', 'e t', 'ii',
                                        'me c', 'a be', 'c r', 'o', 'it'
                                        )))%>%
  mutate(lyrics = lemmatize_strings(lyrics))%>%
  unnest_tokens(word, lyrics, 'ngrams', n=3)%>%
  filter(!word %in% c(tolower(song), 'wu tang'))%>%
  filter(nchar(word) > 2)%>%
  count(word, sort=T)%>%
  slice(1:75)%>%
  ggplot(aes(label=word, size=n,color=log(n)))+
  geom_text_wordcloud()+
  scale_color_continuous(low='yellow', high='black')+
  theme_minimal()+
  labs(title='Most frequent trigrams Wu-tang songs')

grid.arrange(word, trigram, ncol=2)
```

![](wuanalysis_files/figure-markdown_github/unnamed-chunk-4-1.png)

TF-IDF Wordclouds by Song
=========================

I love tf-idf. Here we use it to visualize the 'most unique' words by song. Want to quickly get an idea of the key topics in a song? Use tf-idf.

I didn't include all songs because the facets would get too small. Here's just a taste.

``` r
df%>%
  unnest_tokens(word, lyrics)%>%
  anti_join(stop_words)%>%
  filter(!word %in% tolower(song))%>%
  count(song, word)%>%
  bind_tf_idf(word, song, n)%>%
  group_by(song)%>%
  arrange(desc(tf_idf))%>%
  slice(1:15)%>%
  ungroup()%>%
  slice(1:300)%>%
  ggplot(aes(label=word, size=tf_idf,color=tf_idf))+
  geom_text_wordcloud_area()+
  scale_color_continuous(low='blue', high='red')+
  facet_wrap(~song, ncol=5)+
  theme_minimal()
```


![](wuanalysis_files/figure-markdown_github/unnamed-chunk-5-1.png)

Visualize correlated networks of lyrics by using the phi coefficient
====================================================================

This one was inspired by the great book "Text Mining with R." What we are doing is looking, per song, at pairs of words. We are doing something similar to Pearson's correlation coefficient but looking at counts of times word pairs appear versus times when they do not appear together. Word pairs that often occur together within the same song will be given a high phi correlation (close to 1), while words that never appear together will get negative scores (close to -1).

We can use this correlation to see how certain words and phrases are used within songs and over the corpus of lyrics as a whole. We will tend to send small groups of words commonly found together in certain songs, but then we also can find phrases that are more common to all Wu-tang songs. These, I would posit, are key components of the general Wu-tang lexicon.


``` r
df%>%
  mutate(lyrics = removeWords(lyrics, c('i','my',stop_words$word)))%>%
  mutate(lyrics = lemmatize_strings(lyrics))%>%
  unnest_tokens(word, lyrics, 'ngrams', n=2)%>%
  #separate(word, c("word1", "word2"), sep = " ") %>%
  filter(nchar(word)> 8)%>%
  group_by(word) %>%
  filter(n() >4)%>%
  pairwise_cor(word, song, sort = TRUE)%>%
  filter(correlation > .55)%>%
  graph_from_data_frame() %>%
  ggraph(layout = "igraph", algorithm='nicely') +
  geom_edge_link(aes(color=correlation),show.legend = FALSE) +
  geom_node_point(color='black', size = 2) +
  geom_node_text(aes(label = name), repel=TRUE, segment.alpha=.2, alpha=.5,size=4) +
  scale_color_continuous(low='red', high='blue')+
  theme_void()
```

![](wuanalysis_files/figure-markdown_github/unnamed-chunk-7-1.png)

Analyzing the sentiment of wu-tang songs
========================================

Here I was interested in finding out which songs seem to be be about the most negative and positive topics. For each word in the song, we calculate a sentiment score and assign an index to each word in a song so we can have a temporal ordering of the sentiment during the song. Finally, I have ordered these songs by the summed sentiments: so the first songs in the top left represent the most 'negative' songs, while the songs at the bottom represent the most 'positive' songs (overall). In general, I think you can see a relationship between the title of the song and the overall sentiment. The black line at y=0 represents 'neutral' sentiment.

Unfortunately, I couldn't include all 60+ songs here because the width of the webpage is not big enough to fit the graphic (even though it displays fine on my laptop).

``` r
df%>%
  mutate(lyrics = removeWords(lyrics, c('i','my',stop_words$word)))%>%
  mutate(lyrics = lemmatize_strings(lyrics))%>%
  unnest_tokens(word, lyrics)%>%
  filter(nchar(word) > 2)%>%
  inner_join(get_sentiments('afinn'))%>%
  group_by(song)%>%
  mutate(idx = row_number())%>%
  mutate(total_emo = sum(score))%>%
  ungroup()%>%
  mutate(song = reorder(song, total_emo))%>%
  slice(1:1000)%>%
  ggplot(aes(idx, score,group=song, color=score))+
  geom_hline(yintercept=0)+
  geom_line(size=.7)+
  guides(color=FALSE)+
  scale_color_continuous(low='red', high='blue')+
  theme_minimal()+
  facet_wrap(~ song, ncol=5,scales = 'free')
```

![](wuanalysis_files/figure-markdown_github/unnamed-chunk-8-1.png)

Named entity extraction: locations and people referenced in Wu-tang's songs
===========================================================================

Here's where it gets interesting: what if we could extract all the people and places referred to in Wu-tang lyrics? And what if we could then map these locations to see which places most likely influenced Wu-tang members? I'm particularly interested in doing this for Ghostface Killah's lyrics (or maybe MF Doom), since he's famous for his bizarre pop culture references.

This time--after waaaaaayyy too much time--I finally figured out how to extract the predicted probabilities for the entity annotator. This is great because now we can set a false positive threshold for our classifications. Instead of getting lots of 'junk' locations and people, we can only include those entities that our classifier has deemed have a greater than x% chance of being a person or a place.

``` r
persons <- Maxent_Entity_Annotator(kind='person', probs=T)
locations <- Maxent_Entity_Annotator(kind='location', probs = T)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
```

Remember here we need to collapse everything into one large string.

``` r
# Make one complete document
text = paste(df$lyrics, collapse=' ' )
text = as.String(text)

#create the annotation object that we will use to extract probabilities
ann <- NLP::annotate(text, list(sent_token_annotator, 
                                   word_token_annotator,persons,
                                   locations))

#[[ subsets. the k is looking for kind, then subsetting location kind and person kind
k <- sapply(ann$features, `[[`, "kind")
```

A function that extracts the predicted probabilities and returns a nice dataframe
=================================================================================

This is where things got hairy. Basically, what we wanted to do here was count the entities in order to gauge how important these places or people were. So bigger numbers of mentions represent things more important to Wu-tang members. Second, we average the predicted probabilities for each of these entities for simple plotting.

``` r
get_word_probs <- function(extract_type ='person'){
  prob_person <- sapply(ann[k == extract_type], function(x) x[[1]][1]$features[[1]]$prob)
  persons_list <- text[ann[k ==  extract_type]]
  d <- tibble(extract_type = persons_list, pred_prob = prob_person)
  cts <- d%>%
    group_by(extract_type)%>%
    count(extract_type, sort=T)
  d <- d %>%
  filter(nchar(extract_type) < 20 & nchar(extract_type) > 2)%>%
  unique()%>%
  group_by(extract_type)%>%
  summarize(avg_prob = mean(pred_prob))
  
  d <- d%>%
    inner_join(cts)
  d
  }

#get our DFs for plotting
prob_df <- get_word_probs(extract_type='person')
```


``` r
prob_df_loc <- get_word_probs(extract_type='location')
```


Plotting locations and people vs. confidence in predictions
===========================================================

Here I wanted to show you the people and places mentioned in the lyrics. They are colored by their probability of being a true positive. So words near 1.0 mean the algorithm was quite sure they really were people or places, words near .50 weren't so easy to classify and thus received a less confident vote by the algorithm. Next, the Y axis represents how many times these entities were mentioned in the lyrics. So by looking at this graph we can see which entities were most important and also get a feel for how certain the algorithm was in classfying them.

Keep in mind I had to jitter a little bit to overcome the issue of overplotting the words. So take y-axis value with a grain of salt. Shout out to the Richard Dawkins reference. I'm guessing that was either RZA or Ghostface.

``` r
library(ggrepel)
prob_df%>%
  ggplot(aes(avg_prob,n, size=n, color=avg_prob))+
  geom_text(aes(label=extract_type), alpha=.9, position = position_jitter(width=, height=8))+
  scale_color_continuous(low='blue', high='red')+
  scale_size_continuous(range = c(2,6))+
  guides(size=F, color=F)+
  theme_minimal()+
  labs(title='Counts of person vs. probability of NER person', x='confidence of prediction',y='Times mentioned in lyrics')
```

![](wuanalysis_files/figure-markdown_github/unnamed-chunk-13-1.png)

``` r
prob_df_loc%>%
  ggplot(aes(avg_prob,n, size=n, color=avg_prob))+
  geom_text(aes(label=extract_type), alpha=.9, position = position_jitter(width=, height=3))+
  scale_color_continuous(low='blue', high='red')+
  scale_size_continuous(range = c(2,6))+
  guides(size=F, color=F)+
  theme_minimal()+
  labs(title='Counts of location vs. probability of NER location', x='confidence of prediction',y='Times mentioned in lyrics')
```

![](wuanalysis_files/figure-markdown_github/unnamed-chunk-13-2.png)

Plotting locations mentioned by Wu-tang on a map
================================================

Finally, I wanted to finish by geocoding and mapping all the locations mentioned in the lyrics. Overall I think this approach worked fairly well. Don't forget to zoom out and look at all the international references (Egypt, China, Medina, etc.). The points are sized according to how many times they were mentioned in the text. BROOKLYN REPRESENT!

``` r
library(leaflet)
library(ggmap)
library(htmlwidgets)
```

``` r
geo_loc <- prob_df_loc%>%
  mutate_geocode(extract_type, source='dsk')
```


``` r
word_pal = colorBin('Reds', domain = geo_loc$n, bins=9) 
loc_map <- geo_loc%>%
  leaflet()%>%
  setView(-73.94958, 40.65010, zoom = 10)%>%
  addProviderTiles('OpenStreetMap.BlackAndWhite')%>%
  addCircleMarkers(radius = ~ n, label = ~ extract_type,
                   color= ~ 'red', fillOpacity = .5,
                   fillColor = ~ word_pal(n))
```

``` r
saveWidget(loc_map, file="wu_map.html")
```
{% include wu_map.html %}
